{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7d3bd164",
      "metadata": {
        "id": "7d3bd164"
      },
      "source": [
        "\n",
        "# Multimodal Transformers Demo (CLIP)\n",
        "**Course:** INFO I418 — HONORS PROJECT\n",
        "\n",
        "\n",
        "**Author:** Arash Mousavi\n",
        "\n",
        "**Mentor:** Prof. Bryan Stephens\n",
        "\n",
        "This is a mini demo that allows you lets type one or more text prompts (through captions) and then you can upload an image of it.  \n",
        "It uses a CLIP model from Hugging Face to compute text–image cosine similarities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e3c717a",
      "metadata": {
        "id": "9e3c717a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Imports and model load\n",
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import os\n",
        "import io\n",
        "from typing import List\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# There are other options of model such as laion/CLIP-ViT-B-32-laion2B-s34B-b79K.\n",
        "MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
        "\n",
        "model = CLIPModel.from_pretrained(MODEL_NAME).to(device)\n",
        "processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "870b4421",
      "metadata": {
        "id": "870b4421"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def compute_similarities(image: Image.Image, texts: List[str]):\n",
        "    inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        image_embeds = outputs.image_embeds / outputs.image_embeds.norm(p=2, dim=-1, keepdim=True)\n",
        "        text_embeds = outputs.text_embeds / outputs.text_embeds.norm(p=2, dim=-1, keepdim=True)\n",
        "        # the cosine similarity (batch_size_text x 1)\n",
        "        sims = (text_embeds @ image_embeds.T).squeeze(-1).detach().cpu().numpy()\n",
        "    return sims\n",
        "\n",
        "def plot_bar(similarities, labels, title=\"CLIP Text–Image Similarity\"):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    idx = np.argsort(similarities)[::-1]\n",
        "    sims_sorted = similarities[idx]\n",
        "    labels_sorted = [labels[i] for i in idx]\n",
        "    plt.bar(range(len(sims_sorted)), sims_sorted)\n",
        "    plt.xticks(range(len(sims_sorted)), labels_sorted, rotation=30, ha='right')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.ylabel(\"Cosine similarity\")\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db9f279f",
      "metadata": {
        "id": "db9f279f"
      },
      "outputs": [],
      "source": [
        "from io import BytesIO\n",
        "\n",
        "try:\n",
        "    from google.colab import files as colab_files\n",
        "    colab_env = True\n",
        "except Exception:\n",
        "    colab_env = False\n",
        "\n",
        "uploaded_img = None\n",
        "\n",
        "up = colab_files.upload()\n",
        "\n",
        "name = next(iter(up.keys()))\n",
        "uploaded_img = Image.open(BytesIO(up[name]))\n",
        "\n",
        "\n",
        "caption_input = \"a dog; a shark; a guy riding a bike; a bowl of fruit; Key\"       # CAPTION HERE, TYPE IN THE ANYTHING then run and upload the picture.\n",
        "captions = [c.strip() for c in caption_input.split(\";\") if c.strip()]\n",
        "\n",
        "display(uploaded_img.resize((min(uploaded_img.width, 512), min(uploaded_img.height, 512))))\n",
        "\n",
        "sims = compute_similarities(uploaded_img.convert(\"RGB\"), captions)\n",
        "plot_bar(sims, captions, title=\"CLIP Text–Image similarity\")\n",
        "\n",
        "best_idx = int(np.argmax(sims))\n",
        "print(f\" the most potential match: '{captions[best_idx]}' (similarity={sims[best_idx]:.3f})\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}